\section{CUDA Preperation \& OpenMP}

In this step we will cover the transformations we applied to the code on order
to make it easier to parallelize in the end. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{OpenMP}
The first task was to use OpenMP to parllelize the outermost loop in
\texttt{run\_OrigCPU} by adding an OpenMP pragma to it. The resulting loop can
be seen in Figure \ref{code:openmp1}.

\begin{figure}[H]
    \begin{lstlisting}
#pragma omp parallel for default(shared) schedule(static) if(outer>8)
for( unsigned i = 0; i < outer; ++ i ) {
    REAL strike = 0.001*i;
    PrivGlobs    globs(numX, numY, numT);
    res[i] = value( globs, s0, strike, t,
                    alpha, nu,    beta,
                    numX,  numY,  numT );
}
    \end{lstlisting}
    \caption{The outer loop parallelized using OpenMP.}
    \label{code:openmp1}
\end{figure}

Originally the declerations of \textit{strike} and \textit{globs} that we see in
Figure \ref{code:openmp1} was placed outside the loop, but in order to
parallelize the loop they had to be privatized. If they had stayed outside the
loop each different value of strike and globs would have been mapped to the same
memory location and the different threads would be writing to, and reading from
the same location instead of individual locations. This is needed since the
values for each iteration of the loop are not the same. The above
parallelization is safe because no loop iteration (thread) reads or writes to
any shared variables, and the rest of the code (everything inside the
\texttt{value} function) is executed sequentially.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Array Expansion}

One of the transformatinos we applied to the program was array expansion as
explained in \cite[Slide 11]{projectslide}. Array expansion essentially adds an
extra dimension to whatever data you are working with, but works better than
privatization when parallelizing for a GPU since dynamically allocating memory
in a CUDA kernel is not possible. If we use the loop from Figure
\ref{code:openmp1} as an example, we would allocate an array of
\texttt{PrivGlobs} before starting the loop, and then access each individual
\texttt{PrivGlobs} for each iteration of the loop, the result would look like
the code shown in Figure \ref{code:arrayexp1}. Array expansion is used to
increase the degree of parallelization since we can now run as many threads as
the loop have iterations.

\begin{figure}[H]
    \begin{lstlisting}
PrivGlobs *globs = (PrivGlobs*) malloc(outer*sizeof(struct PrivGlobs));

for(int i = 0 ; i < outer ; i++) { //par
    globs[i] = PrivGlobs(numX,numY,numT);
}

for( unsigned i = 0; i < outer; ++ i ) { //par
        REAL strike = 0.001*i;
        res[i] = value( globs[i], s0, strike, t,
                    alpha, nu,    beta,
                    numX,  numY,  numT );
}
    \end{lstlisting}
    \caption{The outer loop modified to use array expansion instead of
    privatization.}
    \label{code:arrayexp1}
\end{figure}

The code in Figure \ref{code:arrayexp1} is using OpenMP, so the array expansion
is not strictly needed since it can dynamically allocate memory, but the step is
done to prepare the code for being translated to CUDA. This transformation is
applied other places in the code, but this outer loop is the clearest example.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Loop Distribution}

Another transformation we applied was loop distribution as described in
\cite[Slide 13]{projectslide}. This optimization is applied when you have an
outer loop that can be parallelized, with other parallelizable inner loops, that
have some sequential code between them. The outer loop again provided a good
example of this when we look into the \texttt{value} function as well. Figure
\ref{code:predistvalue} shows the \texttt{value} function, it contains some
function calls and sequential loop.

\begin{figure}[H]
    \begin{lstlisting}
REAL   value(   PrivGlobs    globs,
                const REAL s0,
                const REAL strike,
                const REAL t,
                const REAL alpha,
                const REAL nu,
                const REAL beta,
                const unsigned int numX,
                const unsigned int numY,
                const unsigned int numT
) {
    initGrid(s0,alpha,nu,t, numX, numY, numT, globs);
    initOperator(globs.myX,globs.myDxx);
    initOperator(globs.myY,globs.myDyy);

    setPayoff(strike, globs);
    for(int i = globs.myTimeline.size()-2;i>=0;--i) {//seq
        updateParams(i,alpha,beta,nu,globs);
        rollback(i, globs);
    }

    return globs.myResult[globs.myXindex][globs.myYindex];
}
    \end{lstlisting}
    \caption{The \texttt{value} function as it was handed out.}
    \label{code:predistvalue}
\end{figure}

If we pull the code from the \texttt{value} function out into the outer loop
directly, it will look like Figure \ref{code:arrayexp2}, since the functions
\texttt{initGrid} and \texttt{initOperator} as well as \texttt{setPayoff} were
safe to parallelize we reorganized the loops to the form in Figure
\ref{code:arrayexp3}.

\begin{figure}[H]
    \begin{lstlisting}
PrivGlobs *globs = (PrivGlobs*) malloc(outer*sizeof(struct PrivGlobs));

for(int i = 0 ; i < outer ; i++) { //par
    globs[i] = PrivGlobs(numX,numY,numT);
}

for( unsigned i = 0; i < outer; ++ i ) { //par
        REAL strike = 0.001*i;
        initGrid(s0,alpha,nu,t, numX, numY, numT, globs);
        initOperator(globs.myX,globs.myDxx);
        initOperator(globs.myY,globs.myDyy);

        setPayoff(strike, globs);

        for(int j = globs.myTimeline.size()-2;j>=0;--j) //seq
        {
            updateParams(j,alpha,beta,nu,globs);
            rollback(j, globs);
        }

        res[i] = globs[i].myResult[globs.myXindex][globs.myYindex];
}
    \end{lstlisting}
    \caption{The outer loop after value is expanded into it.}
    \label{code:arrayexp2}
\end{figure}

Figure \ref{code:arrayexp3} shows the loops after they were disitributed, this
transformation allowed us to now run all the init operations in parallel, which
increased the degree of parallelization. Furthermore it allowed us to
interchange the sequential inner loop and the parallel outerloop which again
allowed us top increase the degree of parallelism.

\begin{figure}[H]
    \begin{lstlisting}
PrivGlobs *globs = (PrivGlobs*) malloc(outer*sizeof(struct PrivGlobs));

for(int i = 0 ; i < outer ; i++) { //par
    globs[i] = PrivGlobs(numX,numY,numT);
}

for( unsigned i = 0; i < outer; ++ i ) { //par
        initGrid(s0,alpha,nu,t, numX, numY, numT, globs[i]);
        initOperator(globs[i].myX, globs[i].myXsize, globs[i].myDxx, globs[i].myDxxCols);
        initOperator(globs[i].myY, globs[i].myYsize, globs[i].myDyy, globs[i].myDyyCols);
        setPayoff(0.001*i, globs[i]);
}

// sequential loop distributed.
for(int i = numT-2;i>=0;--i){ //seq
    #pragma omp parallel for default(shared) schedule(static) if(outer>8)
    for( unsigned j = 0; j < outer; ++ j ) { //par
        updateParams(i,alpha,beta,nu,globs[j]);
        rollback(i, globs[j]);
    }
}
    \end{lstlisting}
    \caption{The same code as in \ref{code:arrayexp2} but after the loops are
    distributed.}
    \label{code:arrayexp3}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Memory Coalescing}

Since the GPU reads and writes to memory in chunks the way memory is accecssed
is very important for speed. Reading the first element from each row of the
matrix will cause an entire memory chunk to be loaded for each row, if we
instead transpose the matrix so that each read is for an entire row instead, the
chunk will contain more data that we actually need. Ensuring memory coalescing
does not increase the degree og parallelism but it speeds up the code by making
memory operations faster.

\begin{figure}[H]
    \begin{lstlisting}
for(i=0;i<numX;i++) { //par
    for(j=0;j<numY;j++) { //par
        u[j][i] = dtInv*globs.myResult[i][j];

        if(i > 0) {
          u[j][i] += 0.5*( 0.5*globs.myVarX[i][j]*globs.myDxx[i][0] )
                        * globs.myResult[i-1][j];
        }
        u[j][i]  +=  0.5*( 0.5*globs.myVarX[i][j]*globs.myDxx[i][1] )
                        * globs.myResult[i][j];
        if(i < numX-1) {
          u[j][i] += 0.5*( 0.5*globs.myVarX[i][j]*globs.myDxx[i][2] )
                        * globs.myResult[i+1][j];
        }
    }
}
    \end{lstlisting}
    \caption{The first loop of the rollback function. This example is with the
    2D datastructures in the original handout, not the flat structures created
    earlier.}
    \label{code:coal1}
\end{figure}

An example of this can be seen in Figure \ref{code:coal1} which is the forst
loop in the \texttt{rollback} function. In this loop, the read operations are
coalesced as they read from rows at a time, but the writes are for columns. If
the loops are interchanged we can coales the writes but have uncualesced reads,
so the solution is to instead transpose $u$, making the writes coalesced.

\begin{figure}[H]
    \begin{lstlisting}
for(i=0;i<numX;i++) { //par
    for(j=0;j<numY;j++) { //par

        uT[idx2d(i,j,numY)] = dtInv*globs.myResult[idx2d(i,j,globs.myResultCols)];
        if(i > 0) {
            uT[idx2d(i,j,numY)] += 0.5*( 0.5*globs.myVarX[idx2d(i,j,globs.myVarXCols)]*globs.myDxx[idx2d(i,0,globs.myDxxCols)] )
                        * globs.myResult[idx2d(i-1,j,globs.myResultCols)];
        }
        uT[idx2d(i,j,numY)]  += 0.5*( 0.5*globs.myVarX[idx2d(i,j,globs.myVarXCols)]*globs.myDxx[idx2d(i,1,globs.myDxxCols)] )
                        * globs.myResult[idx2d(i,j,globs.myResultCols)];
        if(i < numX-1) {
            uT[idx2d(i,j,numY)] += 0.5*( 0.5*globs.myVarX[idx2d(i,j,globs.myVarXCols)]*globs.myDxx[idx2d(i,2,globs.myDxxCols)] )
                        * globs.myResult[idx2d(i+1,j,globs.myResultCols)];
        }
    }
}
transpose(uT, &u, numY, numX);
    \end{lstlisting}
    \caption{The first loop of the rollback function using a transposed $u$ for
    coalescing writes. This example is from the code after the structures were
    flattened to simple arrays.}
    \label{code:coal2}
\end{figure}

The code in Figure \ref{code:coal2} shows the loops and the following transpose.
In this way, both reads and writes are coalesced and after the transpose
operation in the last line, any code can use $u$ as they did before. The
additional transpose operation does incur an extra cost, but for large matrices
this cost becomes more an more negligable compared to the speedup in the memory
system.