\section{CUDA Preperation \& OpenMP}

In this step we will cover the transformations we applied to the code on order
to make it easier to parallelize in the end. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{OpenMP}
The first task was to use OpenMP to parllelize the outermost loop in
\texttt{run\_OrigCPU} by adding an OpenMP pragma to it. The resulting loop can
be seen in Figure \ref{code:openmp1}.

\begin{figure}[H]
    \begin{lstlisting}
#pragma omp parallel for default(shared) schedule(static) if(outer>8)
for( unsigned i = 0; i < outer; ++ i ) {
    REAL strike = 0.001*i;
    PrivGlobs    globs(numX, numY, numT);
    res[i] = value( globs, s0, strike, t,
                    alpha, nu,    beta,
                    numX,  numY,  numT );
}
    \end{lstlisting}
    \caption{The outer loop parallelized using OpenMP.}
    \label{code:openmp1}
\end{figure}

Originally the declerations of \textit{strike} and \textit{globs} that we see in
Figure \ref{code:openmp1} was placed outside the loop, but in order to
parallelize the loop they had to be privatized. If they had stayed outside the
loop each different value of strike and globs would have been mapped to the same
memory location and the different threads would be writing to, and reading from
the same location instead of individual locations. This is needed since the
values for each iteration of the loop are not the same. The above
parallelization is safe because no loop iteration (thread) reads or writes to
any shared variables, and the rest of the code (everything inside the
\texttt{value} function) is executed sequentially.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Array Expansion}

One of the transformatinos we applied to the program was array expansion as
explained in \cite[Slide 11]{projectslide}. Array expansion essentially adds an
extra dimension to whatever data you are working with, but works better than
privatization when parallelizing for a GPU since dynamically allocating memory
in a CUDA kernel is not possible. If we use the loop from Figure
\ref{code:openmp1} as an example, we would allocate an array of
\texttt{PrivGlobs} before starting the loop, and then access each individual
\texttt{PrivGlobs} for each iteration of the loop, the result would look like
the code shown in Figure \ref{code:arrayexp1}. Array expansion is used to
increase the degree of parallelization since we can now run as many threads as
the loop have iterations.

\begin{figure}[H]
    \begin{lstlisting}
PrivGlobs *globs = (PrivGlobs*) malloc(outer*sizeof(struct PrivGlobs));

for(int i = 0 ; i < outer ; i++) { //par
    globs[i] = PrivGlobs(numX,numY,numT);
}

for( unsigned i = 0; i < outer; ++ i ) { //par
        REAL strike = 0.001*i;
        res[i] = value( globs[i], s0, strike, t,
                    alpha, nu,    beta,
                    numX,  numY,  numT );
}
    \end{lstlisting}
    \caption{The outer loop modified to use array expansion instead of
    privatization.}
    \label{code:arrayexp1}
\end{figure}

The code in Figure \ref{code:arrayexp1} is using OpenMP, so the array expansion
is not strictly needed since it can dynamically allocate memory, but the step is
done to prepare the code for being translated to CUDA. This transformation is
applied other places in the code, but this outer loop is the clearest example.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Loop Distribution}

Another transformation we applied was loop distribution as described in
\cite[Slide 13]{projectslide}. This optimization is applied when you have an
outer loop that can be parallelized, with other parallelizable inner loops, that
have some sequential code between them. The outer loop again provided a good
example of this when we look into the \texttt{value} function as well. Figure
\ref{code:predistvalue} shows the \texttt{value} function, it contains some
function calls and sequential loop.

\begin{figure}[H]
    \begin{lstlisting}
REAL   value(   PrivGlobs    globs,
                const REAL s0,
                const REAL strike,
                const REAL t,
                const REAL alpha,
                const REAL nu,
                const REAL beta,
                const unsigned int numX,
                const unsigned int numY,
                const unsigned int numT
) {
    initGrid(s0,alpha,nu,t, numX, numY, numT, globs);
    initOperator(globs.myX,globs.myDxx);
    initOperator(globs.myY,globs.myDyy);

    setPayoff(strike, globs);
    for(int i = globs.myTimeline.size()-2;i>=0;--i) {//seq
        updateParams(i,alpha,beta,nu,globs);
        rollback(i, globs);
    }

    return globs.myResult[globs.myXindex][globs.myYindex];
}
    \end{lstlisting}
    \caption{The \texttt{value} function as it was handed out.}
    \label{code:predistvalue}
\end{figure}

If we pull the code from the \texttt{value} function out into the outer loop
directly, it will look like Figure \ref{code:arrayexp2}, since the functions
\texttt{initGrid} and \texttt{initOperator} as well as \texttt{setPayoff} were
safe to parallelize we reorganized the loops to the form in Figure
\ref{code:arrayexp3}.

\begin{figure}[H]
    \begin{lstlisting}
PrivGlobs *globs = (PrivGlobs*) malloc(outer*sizeof(struct PrivGlobs));

for(int i = 0 ; i < outer ; i++) { //par
    globs[i] = PrivGlobs(numX,numY,numT);
}

for( unsigned i = 0; i < outer; ++ i ) { //par
        REAL strike = 0.001*i;
        initGrid(s0,alpha,nu,t, numX, numY, numT, globs);
        initOperator(globs.myX,globs.myDxx);
        initOperator(globs.myY,globs.myDyy);

        setPayoff(strike, globs);

        for(int j = globs.myTimeline.size()-2;j>=0;--j) //seq
        {
            updateParams(j,alpha,beta,nu,globs);
            rollback(j, globs);
        }

        res[i] = globs[i].myResult[globs.myXindex][globs.myYindex];
}
    \end{lstlisting}
    \caption{The outer loop after value is expanded into it.}
    \label{code:arrayexp2}
\end{figure}

Figure \ref{code:arrayexp3} shows the loops after they were disitributed, this
transformation allowed us to now run all the init operations in parallel, which
increased the degree of parallelization. Furthermore it allowed us to
interchange the sequential inner loop and the parallel outerloop which again
allowed us top increase the degree of parallelism.

\begin{figure}[H]
    \begin{lstlisting}
PrivGlobs *globs = (PrivGlobs*) malloc(outer*sizeof(struct PrivGlobs));

for(int i = 0 ; i < outer ; i++) { //par
    globs[i] = PrivGlobs(numX,numY,numT);
}

for( unsigned i = 0; i < outer; ++ i ) { //par
        initGrid(s0,alpha,nu,t, numX, numY, numT, globs[i]);
        initOperator(globs[i].myX, globs[i].myXsize, globs[i].myDxx, globs[i].myDxxCols);
        initOperator(globs[i].myY, globs[i].myYsize, globs[i].myDyy, globs[i].myDyyCols);
        setPayoff(0.001*i, globs[i]);
}

// sequential loop distributed.
for(int i = numT-2;i>=0;--i){ //seq
    #pragma omp parallel for default(shared) schedule(static) if(outer>8)
    for( unsigned j = 0; j < outer; ++ j ) { //par
        updateParams(i,alpha,beta,nu,globs[j]);
        rollback(i, globs[j]);
    }
}
    \end{lstlisting}
    \caption{The same code as in \ref{code:arrayexp2} but after the loops are
    distributed.}
    \label{code:arrayexp3}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Memory Coalescing}