\documentclass[xcolor=x11names,compress]{beamer}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ucs}
\usepackage[danish]{babel}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{minted}

\setcounter{tocdepth}{1}

\usepackage{etoolbox}
% \AtBeginEnvironment{minted}{\singlespacing \fontsize{10}{10}\selectfont}


\usepackage{tikz}
\usetikzlibrary{decorations.fractals}
\useoutertheme[subsection=false,shadow]{miniframes}
\useinnertheme{default}

\usefonttheme{serif}
\usepackage{palatino}

\setbeamerfont{title like}{shape=\scshape}
\setbeamerfont{frametitle}{shape=\scshape}

\setbeamercolor*{lower separation line head}{bg=DeepSkyBlue4} 
\setbeamercolor*{normal text}{fg=black,bg=white} 
\setbeamercolor*{alerted text}{fg=red} 
\setbeamercolor*{example text}{fg=black} 
\setbeamercolor*{structure}{fg=black} 
 
\setbeamercolor*{palette tertiary}{fg=black,bg=black!10} 
\setbeamercolor*{palette quaternary}{fg=black,bg=black!10} 


\begin{document}
\title{Programming Massively Parallel Hardware} 
\author{
    Martin JÃ¸rgensen \& Henrik Bendt
} 
\date{\today} 

\frame{\titlepage} 

\frame{\frametitle{Table of contents}\tableofcontents} 

\section{Introduction}
\frame{\frametitle{Introduction} 
Goal: Parallelize given system.

Methods: 
\begin{itemize}
    \item Flattening vectors
    \item Privatization (CPU)
    \item Vector to array
    \item Array expansion
    \item Inline scalar variables
    \item Loop distribution
    \item Coalesced memory access
\end{itemize}
}

\frame{\frametitle{Introduction} 
Parallelization not fully done due to mismanagement of time/focus. We lost at least 20 hours.

Should have done smaller steps converting to CUDA, and then optimized on this.

That is, made smaller naive kernels to transition to CUDA earlier, and then optimized. 

This would also have made benchmarking better, as each attempted optimization could have been measured against a naive version.
}



\section{Data Structure Flattening}

\subsection{Matrix to Vector}
%\frame{\frametitle{Matrix to Vector - Declaration} 
\begin{frame}[fragile]{Matrix to Vector - Declaration}
\begin{minted}{cpp}
vector<vector<REAL> > myResult; // [numX][numY]
//...
this->myResult.resize(numX);
for(unsigned i=0;i<numX;++i) {
    this->myResult[i].resize(numY);
}
\end{minted}
\end{frame}

\begin{frame}[fragile]{Matrix to Vector - Declaration}
\begin{minted}{cpp}
vector<REAL> myResult; // [numX][numY]
unsigned myResultRows;
unsigned myResultCols;
//...
this->myResult.resize(numX*numY);
this->myResultRows = numX;
this->myResultCols = numY;
\end{minted}
\end{frame}

\begin{frame}[fragile]{Matrix to Vector - Usage}
\begin{minted}{cpp}
for(unsigned i=0 ;
    i < globs.myX.size() ;
    ++i) {
    for(unsigned j=0 ;
        j < globs.myY.size() ;
        ++j) 
        globs.myResult[i][j] = payoff[i];
}
\end{minted}
\end{frame}

\begin{frame}[fragile]{Matrix to Vector - Usage}
\begin{minted}{cpp}
for(unsigned i=0 ;
    i < globs.myXsize ;
    ++i) {
    for(unsigned j=0 ;
        j < globs.myYsize ;
        ++j) 
        globs.myResult[
            idx2d(i,j,globs.myResultCols)]
            = payoff[i];
}
\end{minted}
\end{frame}


\subsection{Vector to Array}
\begin{frame}[fragile]{Vector to Array - Declaration}
\begin{minted}{cpp}
vector<REAL> myResult; // [numX][numY]
unsigned myResultRows;
unsigned myResultCols;
//...
this->myResult.resize(numX*numY);
this->myResultRows = numX;
this->myResultCols = numY;
\end{minted}
\end{frame}

\begin{frame}[fragile]{Vector to Array - Declaration}
\begin{minted}{cpp}
REAL* myResult; // [numX][numY]
unsigned myResultRows;
unsigned myResultCols;
//...
this->myResult = (REAL*) malloc(sizeof(REAL) *
                                numX*numY);
this->myResultRows = numX;
this->myResultCols = numY;
\end{minted}
\end{frame}

\subsection{Vector to Array}
\frame{\frametitle{Vector to Array - Usage} 
    \noindent\texttt{void initOperator(const vector<REAL> \&x, unsgined xSize,
    vector<REAL> \&Dxx, unsigned DxxCols );}

    \noindent\texttt{void initOperator( const REAL* \&x, unsgined xSize,
    REAL* \&Dxx, unsigned DxxCols );}
}


\section{CUDA Preperation}
\subsection{Array Expansion}
\begin{frame}[fragile]{Array Expansion}
        \begin{minted}{cpp}
// a = [numX]
for(j=0;j<numY;j++) { //seq
    for(i=0;i<numX;i++) { //par
        a[i] =  ...
    }
    ... //code using a
}
\end{minted}
\end{frame}

\begin{frame}[fragile]{Array Expansion}
\begin{minted}{cpp}
// a = [numY][numX]
for(j=0;j<numY;j++) { //par
    for(i=0;i<numX;i++) { //par
        a[j][i] =  ...
    }
    ...//code using a
}
\end{minted}
\end{frame}

\subsection{Loop Distribution}
\begin{frame}[fragile]{Loop Distribution - Before}
\begin{minted}[fontsize=\tiny]{cpp}
PrivGlobs *globs = (PrivGlobs*) malloc(outer*sizeof(struct PrivGlobs));

for(int i = 0 ; i < outer ; i++) { //par
    globs[i] = PrivGlobs(numX,numY,numT);
}

for( unsigned i = 0; i < outer; ++ i ) { //par
        REAL strike = 0.001*i;
        initGrid(s0,alpha,nu,t, numX, numY, numT, globs);
        initOperator(globs.myX,globs.myDxx);
        initOperator(globs.myY,globs.myDyy);

        setPayoff(strike, globs);

        for(int j = globs.myTimeline.size()-2;j>=0;--j) //seq
        {
            updateParams(j,alpha,beta,nu,globs);
            rollback(j, globs);
        }

        res[i] = globs[i].myResult[globs.myXindex][globs.myYindex];
}
\end{minted}
\end{frame}

\subsection{Loop Distribution}
\begin{frame}[fragile]{Loop Distribution - After}
\begin{minted}[fontsize=\tiny]{cpp}
PrivGlobs *globs = (PrivGlobs*) malloc(outer*sizeof(struct PrivGlobs));

for(int i = 0 ; i < outer ; i++) { //par
    globs[i] = PrivGlobs(numX,numY,numT);
}

for( unsigned i = 0; i < outer; ++ i ) { //par
        initGrid(s0,alpha,nu,t, numX, numY, numT, globs[i]);
        initOperator(globs[i].myX, globs[i].myXsize, globs[i].myDxx, globs[i].myDxxCols);
        initOperator(globs[i].myY, globs[i].myYsize, globs[i].myDyy, globs[i].myDyyCols);
        setPayoff(0.001*i, globs[i]);
}

// sequential loop distributed.
for(int i = numT-2;i>=0;--i){ //seq
    #pragma omp parallel for default(shared) schedule(static) if(outer>8)
    for( unsigned j = 0; j < outer; ++ j ) { //par
        updateParams(i,alpha,beta,nu,globs[j]);
        rollback(i, globs[j]);
    }
}

for(unsigned i = 0; i < outer; ++ i){ //par
    res[i] = globs[i].myResult[globs.myXindex][globs.myYindex];
}

\end{minted}
\end{frame}

\subsection{Memory Coalescing}
\begin{frame}[fragile]{Memory Coalescing - Before}
\begin{minted}[fontsize=\tiny]{cpp}
for(i=0;i<numX;i++) { //par
    for(j=0;j<numY;j++) { //par
        u[j][i] = dtInv*globs.myResult[i][j];

        if(i > 0) {
          u[j][i] += 0.5*( 0.5*globs.myVarX[i][j]*globs.myDxx[i][0] )
                        * globs.myResult[i-1][j];
        }
        u[j][i]  +=  0.5*( 0.5*globs.myVarX[i][j]*globs.myDxx[i][1] )
                        * globs.myResult[i][j];
        if(i < numX-1) {
          u[j][i] += 0.5*( 0.5*globs.myVarX[i][j]*globs.myDxx[i][2] )
                        * globs.myResult[i+1][j];
        }
    }
}
\end{minted}
\end{frame}

\subsection{Memory Coalescing}
\begin{frame}[fragile]{Memory Coalescing - After}
\begin{minted}[fontsize=\tiny]{cpp}
for(i=0;i<numX;i++) { //par
    for(j=0;j<numY;j++) { //par

        uT[idx2d(i,j,numY)] = dtInv*globs.myResult[idx2d(i,j,globs.myResultCols)];
        if(i > 0) {
            uT[idx2d(i,j,numY)] += /*..*/ globs.myResult[idx2d(i-1,j,globs.myResultCols)];
        }
        uT[idx2d(i,j,numY)]  += /*..*/ globs.myResult[idx2d(i,j,globs.myResultCols)];
        if(i < numX-1) {
            uT[idx2d(i,j,numY)] += /*..*/ globs.myResult[idx2d(i+1,j,globs.myResultCols)];
        }
    }
}
transpose(uT, &u, numY, numX);
\end{minted}
\end{frame}

\section{CUDA Translation}
\frame{\frametitle{CUDA Rewrite}
\begin{enumerate}
   \item PrivGlobs
   \item Initialization
   \item UpdateParams
   \item Rollback - Sequential
   \item Tridag - Sequential
\end{enumerate}
}

\section{Tridag Optimization}
\frame{\frametitle{Tridag Solver Optimized}
\texttt{TRIDAG\_SOLVER} optimized on given test setup.

Avg improvement to original over 5 runs (all validate): 
\[> 300 \mu s\]

}

\subsection{}
\begin{frame}[fragile]{Tridag Solver shared mem 1}
\begin{minted}[fontsize=\tiny]{cpp}
const REAL a_gid = a[gid]; //hold in registers/constant. Accessed twice.
// total shared memory (declared outside)
extern __shared__ char sh_mem[];
// shared memory space for the flag array
volatile int*     flg_sh = (volatile int*    ) sh_mem; //(lin_sh + blockDim.x);
//given c array moved to shared; Read trice on different indices.
volatile REAL* c_arr = (volatile REAL*) (flg_sh + blockDim.x);
//given r array moved to shared; Read twice on different indices.
volatile REAL* r_arr = (volatile REAL*) (c_arr + blockDim.x);
//given b array moved to shared; Read twice on different indices.
volatile REAL* b_arr = (volatile REAL*) (r_arr + blockDim.x);
// shared memory space for the 2x2 matrix multiplication SCAN. THIS WILL BE OVERWRITTEN after use
volatile MyReal4* mat_sh = (volatile MyReal4*) (b_arr + blockDim.x); //sh_mem;
// shared memory space for the linear-function composition SCAN    
\end{minted}
\end{frame}

\begin{frame}[fragile]{Tridag Solver shared mem 2}
\begin{minted}[fontsize=\tiny]{cpp}
flg_sh[tid] = (tid % sgm_sz == 0) ? 1 : 0;
if(gid < n) {
    c_arr[tid] = c[gid];
    r_arr[tid] = r[gid];
    b_arr[tid] = b[gid];
}
__syncthreads();

const unsigned beg_seg_ind_tid = (tid / sgm_sz) * sgm_sz; //on tid 
const REAL b0 = (gid < n) ? b_arr[beg_seg_ind_tid] : 1.0;
mat_sh[tid] = (tid!=beg_seg_ind_tid && gid < n) ?
                //MyReal4(b_arr[tid], -a_gid*c_arr[tid-1], 1.0, 0.0) :
                MyReal4((float) b_arr[tid], -a_gid*c_arr[tid-1], 1.0, 0.0) :
                MyReal4(1.0,                 0.0, 0.0, 1.0) ;
__syncthreads();
\end{minted}
\end{frame}

\begin{frame}[fragile]{Tridag Solver shared mem 3}
\begin{minted}[fontsize=\tiny]{cpp}
volatile MyReal2* lin_sh = (volatile MyReal2*) (r_arr + blockDim.x);
volatile REAL* uu_sh = (volatile REAL*) (lin_sh + blockDim.x);
volatile REAL* u_sh = (volatile REAL*) (uu_sh + blockDim.x);

//[...]

if(gid < n) {
    //uu[gid] = (res4.x*b0 + res4.y) / (res4.z*b0 + res4.w) ;
    uu_sh[tid] = (res4.x*b0 + res4.y) / (res4.z*b0 + res4.w) ;
}

//[...]

const unsigned end_seg_ind_tid = sgm_sz-1+beg_seg_ind_tid;
const unsigned int k_tid = (end_seg_ind_tid - tid) + beg_seg_ind_tid;  
const REAL yn = u_sh[end_seg_ind_tid] / uu_sh[end_seg_ind_tid];

lin_sh[tid] = (tid!=beg_seg_ind_tid && gid < n) ?
                MyReal2( u_sh[k_tid]/uu_sh[k_tid], -c_arr[k_tid]/uu_sh[k_tid] ) :
                MyReal2( 0.0,        1.0         ) ;
__syncthreads();
\end{minted}
\end{frame}


\section{Benchmarks}
\frame{\frametitle{Benchmarks} 
\begin{table}[H]
    \centering
    \begin{tabular}{r|r|r|r}
                             &  S Dataset & M Dataset &  L Dataset \\ \hline
        Sequential Handout   & $2243 ms$ & $4926 ms$ & $244242 ms$ \\
        OMP Handout          &  $204 ms$ &  $324 ms$ &  $10555 ms$ \\
        OMP + Preperation    & $1594 ms$ & $1803 ms$ &  $24766 ms$ \\
        OMP + Flattening     & $1411 ms$ & $1827 ms$ &  $20222 ms$ \\
        CUDA (Unfinished)    & $2744 ms$ & $4329 ms$ & $129250 ms$ \\
    \end{tabular}
    \caption{OMP = OpenMP. Benchmark results for running the different iteration of the code
        on each of the datasets. All times are an average of 5 runs measured in
        microseconds.}
    \label{tab:benchmarks}
\end{table}
}

\section{Conclusion}
\frame{\frametitle{Conclusion} 
We did not make a full CUDA implementation; the essential part we did not get to work.

This was due to mismanagement of the project: making an all-to-CUDA-at-once was not a good approach.

We did however get through all the methods for parallelizing and optimizing the project and got some parts of the program to run on CUDA.

We did not get to optimizing the \texttt{TRIDAG\_SOLVER} in the project, but did this afterwards.
}



\end{document}
